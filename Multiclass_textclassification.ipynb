{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project3 Working.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "12QVdvilws8X",
        "z4u0B1BpwyB7",
        "exPIXvsPx3qz",
        "QQnJ2D2D9KHA",
        "jGdko_L_9M9q"
      ],
      "authorship_tag": "ABX9TyPgRBUPkbCHUj6kFo49ZZch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robitussin/mental-illness/blob/main/Multiclass_textclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c05_nlp_tweaking_the_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c05_nlp_tweaking_the_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "dL3d2Z4FFCau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "12QVdvilws8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxj7svGid4ds",
        "outputId": "a647c132-a867-4edf-f853-fa0e7d01f847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "z4u0B1BpwyB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1: Download dataset from github repository"
      ],
      "metadata": {
        "id": "wrZfuy1Xvkbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/robitussin/mental-illness/blob/4719afa5e9c628e8acd9f4f4d424de74a0481bdb/trainingset.csv?raw=true\"\n",
        "\n",
        "# Load training dataset\n",
        "train_df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "uUvxr7PEtyEo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/robitussin/mental-illness/blob/4719afa5e9c628e8acd9f4f4d424de74a0481bdb/validationset.csv?raw=true\"\n",
        "\n",
        "#Load validation dataset\n",
        "val_df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "rwEwIKshu8mu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 2:\n",
        "Get dataset directly from google drive"
      ],
      "metadata": {
        "id": "_cm0LuddvT0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IlhW3YgneCdo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df = pd.read_csv(\"/content/drive/My Drive/datasets/deeplearning/trainingset.csv\")\n",
        "#print(train_df.info())"
      ],
      "metadata": {
        "id": "FGDeNEqgeGDJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#val_df = pd.read_csv(\"/content/drive/My Drive/datasets/deeplearning/validationset.csv\")\n",
        "#print(val_df.info())"
      ],
      "metadata": {
        "id": "1PjLDxC-emxa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Data"
      ],
      "metadata": {
        "id": "exPIXvsPx3qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge train and test dataset"
      ],
      "metadata": {
        "id": "Ohp3lgH97fZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([train_df,val_df])"
      ],
      "metadata": {
        "id": "KQz_B1HVx3AX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the summary information of the dataset"
      ],
      "metadata": {
        "id": "jxcJkFvyyEmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ2CuwrsihQy",
        "outputId": "2bef6faf-4b4e-4e6a-f8e4-bb6ca2689593"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 15215 entries, 0 to 1487\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   ID          15215 non-null  object\n",
            " 1   title       15215 non-null  object\n",
            " 2   post        15215 non-null  object\n",
            " 3   class_name  15215 non-null  object\n",
            " 4   class_id    15215 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 713.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the different types of labels in the dataset"
      ],
      "metadata": {
        "id": "H2201EJ0yvjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.class_name.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S90-Oc_pysqz",
        "outputId": "d3716c3e-96ef-465b-cbf3-403a9ee9f7b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "adhd          2713\n",
              "depression    2698\n",
              "anxiety       2670\n",
              "bipolar       2655\n",
              "ptsd          2249\n",
              "none          2230\n",
              "Name: class_name, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle the data set"
      ],
      "metadata": {
        "id": "vdtjDxATyQdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1)"
      ],
      "metadata": {
        "id": "PllwTMkDfB_f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzjzoV55fLAL",
        "outputId": "9dfb89e8-faa5-490b-c46d-65727f5d61a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 15215 entries, 7692 to 6818\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   ID          15215 non-null  object\n",
            " 1   title       15215 non-null  object\n",
            " 2   post        15215 non-null  object\n",
            " 3   class_name  15215 non-null  object\n",
            " 4   class_id    15215 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 713.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the first 5 rows of the data set"
      ],
      "metadata": {
        "id": "nexa4Tzay6_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dDWNblpSfdCB",
        "outputId": "114e75aa-d2da-4f5a-cc3d-3d3b17a8a388"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         ID  \\\n",
              "7692   bc914729-3fe2-4b12-8959-52f6878dd448   \n",
              "1127   e2cac13e-dd9e-45bd-8d3d-bb8b1fba1797   \n",
              "12113  59e1d412-36cb-47b8-ba45-80ae84f1daa9   \n",
              "10681  6973c417-a134-402f-bcf0-adf2ca8bbac5   \n",
              "8486   08bbe971-b3d0-4702-899a-5fbac7e2d296   \n",
              "\n",
              "                                                   title  \\\n",
              "7692   when i found \"driven by distraction\" in my pub...   \n",
              "1127                          everyone. just a reminder.   \n",
              "12113  sometimes i wish i could get killed so people ...   \n",
              "10681                                     active shooter   \n",
              "8486   up from a nightmare at 6 am *domestic violence...   \n",
              "\n",
              "                                                    post  class_name  class_id  \n",
              "7692   i'm glad i'm not alone in this. you all make t...        adhd         0  \n",
              "1127   hide your posts so when you comment on somethi...        ptsd         4  \n",
              "12113  sorry if this is the wrong place. i don't thin...  depression         3  \n",
              "10681  i don’t know exactly what to say. i responded ...        ptsd         4  \n",
              "8486   sorry no one will probably read this but thank...        ptsd         4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cd8cfc6-4b01-4dcd-8d5e-a1ab99019420\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>post</th>\n",
              "      <th>class_name</th>\n",
              "      <th>class_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7692</th>\n",
              "      <td>bc914729-3fe2-4b12-8959-52f6878dd448</td>\n",
              "      <td>when i found \"driven by distraction\" in my pub...</td>\n",
              "      <td>i'm glad i'm not alone in this. you all make t...</td>\n",
              "      <td>adhd</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>e2cac13e-dd9e-45bd-8d3d-bb8b1fba1797</td>\n",
              "      <td>everyone. just a reminder.</td>\n",
              "      <td>hide your posts so when you comment on somethi...</td>\n",
              "      <td>ptsd</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12113</th>\n",
              "      <td>59e1d412-36cb-47b8-ba45-80ae84f1daa9</td>\n",
              "      <td>sometimes i wish i could get killed so people ...</td>\n",
              "      <td>sorry if this is the wrong place. i don't thin...</td>\n",
              "      <td>depression</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10681</th>\n",
              "      <td>6973c417-a134-402f-bcf0-adf2ca8bbac5</td>\n",
              "      <td>active shooter</td>\n",
              "      <td>i don’t know exactly what to say. i responded ...</td>\n",
              "      <td>ptsd</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8486</th>\n",
              "      <td>08bbe971-b3d0-4702-899a-5fbac7e2d296</td>\n",
              "      <td>up from a nightmare at 6 am *domestic violence...</td>\n",
              "      <td>sorry no one will probably read this but thank...</td>\n",
              "      <td>ptsd</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cd8cfc6-4b01-4dcd-8d5e-a1ab99019420')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0cd8cfc6-4b01-4dcd-8d5e-a1ab99019420 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0cd8cfc6-4b01-4dcd-8d5e-a1ab99019420');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check all classes"
      ],
      "metadata": {
        "id": "jv7XZoCO71Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['class_name']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmnA3pBB703W",
        "outputId": "40956f3a-1aad-41e8-aedc-71bc89fe2fc2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7692           adhd\n",
              "1127           ptsd\n",
              "12113    depression\n",
              "10681          ptsd\n",
              "8486           ptsd\n",
              "            ...    \n",
              "7386           adhd\n",
              "3682        bipolar\n",
              "3790     depression\n",
              "4830           none\n",
              "6818           none\n",
              "Name: class_name, Length: 15215, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "RgzIXjps8ISr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove unnecessary columns in the data set"
      ],
      "metadata": {
        "id": "r0akOT6EzR_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns=['ID', 'title'])"
      ],
      "metadata": {
        "id": "z5SBskyufm2X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove rows with NULL values"
      ],
      "metadata": {
        "id": "XpBx7Ph2za-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "qlWHkT7UvhM-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a dictionary for id to text label/category"
      ],
      "metadata": {
        "id": "tvQm0moWzo4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_category = pd.Series(data.class_name.values,index=data.class_id).to_dict()\n",
        "id_to_category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNxZVBBDgYn9",
        "outputId": "e8d77029-2d86-4eb0-b1b5-957eb4ed0f87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'adhd', 1: 'anxiety', 2: 'bipolar', 3: 'depression', 4: 'ptsd', 5: 'none'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set limit for testing"
      ],
      "metadata": {
        "id": "vqQO9OFqR8BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#limit the number of samples to be used in testing the pipeline\n",
        "#data_size= 12\n",
        "#data = data[:data_size]\n",
        "#data.info()"
      ],
      "metadata": {
        "id": "sXeY0xzZgjn1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Raw Dataset into Train and Test Datasets"
      ],
      "metadata": {
        "id": "y8Oac9-V0Egy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, targets = data['post'], data['class_id']\n",
        "\n",
        "train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "        features, targets,\n",
        "        train_size=0.8,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        shuffle = True,\n",
        "        stratify=targets\n",
        "    )"
      ],
      "metadata": {
        "id": "M16gXGA1grE-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display first 5 elements of posts"
      ],
      "metadata": {
        "id": "2jeILd-5jWlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.values[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5poWbaNjX2N",
        "outputId": "e985b2bc-2d6c-4c35-e434-e01101906d16"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['i was wondering where i could find an hourly dataset of a climate station of a specific county. the ideal dataset will be **hourly**, and will have information like wind speed, wind direction, atmospheric pressure, humidity, temperature, cloud type, and rainfall. i know that there are climate stations that collect this data but i can only find monthly or daily datasets.',\n",
              "       'so i had a normal childhood up until the point that my parents got divorced. this was in february of \\'97 - i was only six years old during that event (born in the latter half of \\'90). after the divorce is when things kind of went haywire. my mother and father were both alcoholics - they got divorced, yet stayed together (kind of weird? said it was to help me..okay..?) up until 2006 - when i was 15. having said that; here goes. during my childhood between 6 and 15 i experienced a lot of negative things. my father would get black-out drunk - when he did, i would get terrified (this was daily) because of his actions and statements towards me. it wasn\\'t uncommon for me to be threatened with firearms, hatchets, sledgehammers and the like. as an example, a lot of the time he would grab my left chest region and tell me that he could rip my heart out so easily. he was extremely verbally abusive, telling me that he wished i had never been born, that i was a terrible son and that he wanted to disown me. it happened so often that there aren\\'t any memorable scenarios of it happening due to them running together - usually this would happen in the backyard, the garage or in our basement. it never happened on the first or second floor of the household, barring one time where he was loading a gun at the kitchen table stating he would murder all of us. a family member living at the residence at the time phone the police, they showed up with a healthy nine or ten squad cars (lots of police, was basically swat), pulling him outside and pinning a lot of the family to the ground whilst asking questions. my witness statement put him in jail (it was for like 30 or 60 days, wasn\\'t very long). obviously i was a kid and had no idea what was going on and i went to the jail to visit him at least once a week (i had to have been ~8). on the mother side of things, they obviously went to bars all the time. they frequently wouldn\\'t come home until extremely late. (they as in ma &amp; pa) i had to call the bar and ask for my mother by her first name to beg for food. she wouldn\\'t bring it - trying to pawn it off on others. she would eventually bring home food - at like 3 a.m. (when bars close, :x) when the food would be cold and soggy from the grease soaking into it. this happened pretty often and i never really paid no mind to it, once again, i was a kid and didn\\'t really think anything of it. i remember her taking me to my grandparents (her side) for the weekend quite a few times - and being horribly saddened by just being dumped off by her so she could go see different men. once i was fifteen (february of \\'06) i got into an argument with my father and i wasn\\'t taking his bs anymore. i was so full of rage, hatred and disgust that i told him to get the hell out of the house - he did. he went to live with his parents. my mother and i went to live with my grandmother. i really never thought about any of this up until last year when i was about to turn 26. i had blocked it out as a kid, i didn\\'t know what else to do. i never told anybody - i never talked to anybody - i bottled it up and held onto it. not even my best friends knew. all of a sudden i started thinking about it and remembering more and more details about the incidents. as an adult - my life has been a complete and utter mess. i always quit/lose my job under a year into them. i get to the point where i can\\'t handle it any more, i break down, start getting depressed and not wanting to deal with anything. ever since all of this happened i have had habits of thoroughly examining the environments i am in - checking everywhere for anything so that if something were to happen i could be ready for it. i don\\'t trust anybody - i am full of a rage and hatred that tears me apart because i can\\'t cope or deal with people. as a kid and even now i have a lot of trouble sleeping. i sleep very lightly and don\\'t reach rem sleep. i\\'m fantasizing about how other people feel about me or how i feel about them a good 4/5th of the time. it\\'s off the wall crazy stuff as well - being worthless, not being worth anybody\\'s time, incapable of ever being loved - of ever being valued, etc. i\\'m not suicidal but i have grim thoughts like, if i were to get into a traffic accident (because they do happen yknow?) or something that they might finally feel guilty or that my life might have finally meant something. i never think about the positive - only the negative. my weirdest behavior is probably that i\\'m contempt with staring at a wall and/or sitting completely still not doing anything. i spend so much time in my mind that even things like intimacy are beyond me. (i get \"bored\" because i\\'m too busy fantasizing about negative relationships, etc). i get extremely clingy and my fantasizing is about needing that perfect job, having all this money or having this perfect wife who can make me feel like i matter. i overreact to what most adults consider minor stresses - i get into an uncontrollable rage and it is bad. it has almost landed me a felony charge before but i had female friends with me who got me away from the issue to protect me from that. i always feel shameful, guilty - like i\\'ve done something wrong. what did i do to ever deserve this? when it comes to certain activities such as driving, i am often going 100 in a 60, 50 in a 25, etc. i can\\'t help it - it just happens and i try not to, i really do, but it\\'s only when i have passengers that i can refrain from automatically driving like an idiot. having said that, i\\'ve never had a ticket or crashed - which is surprising, given that i fantasize while driving. **edit** i forgot to mention that for most of my life i had an irrational fear of driving and i also had an irrational fear that my parents would leave me at a store a good distance from home and i would never get home. i finally got over the driving thing once i was about 21? i would shake, sweat (we\\'re talking entire pools of sweat), be unable to concentrate, etc. that might have come from the fact that my father used to drink extremely heavily and drive with me in the vehicle (he would literally drink while driving as well) my friends say that i\\'m extremely irritable and easily agitated. they tell me that my mood flip flops all over the place and that sometimes it\\'s hard to deal with me. sometimes i will talk to them or want to do things with them but often i just withdraw entirely, not wanting to leave the house, not wanting to deal with society - and it\\'s gotten so bad that i have to force myself to even do simple things like take my blood pressure medications. i feel like i\\'m always on edge and my heart is always racing - i feel alone and like it\\'s me against the world. i\\'m extremely lethargic (doubly so now that i have diagnosis for autoimmune disorders that cause full body inflammation), random binge-eating sessions followed by not eating at all, no motivation, i\\'m a huge tech nerd that has no interest in tech/games (gamer obviously) for weeks on end every month. there are times when i can\\'t remember something as simple as my birth-date right off hand and have to actually think about it (i think of this one as my mind just being so spread out that i have to purposely try to remember this). the weirdest part of all of this is that for a good six months or so i can put up a front that i\\'m a normal human being - doing wonderfully at jobs but deep down inside i\\'m in an eternal war with myself. i feel like there\\'s a dark version of me and that like right now, i\\'m very much normal - but at the flip of a switch i\\'m completely erratic, hysterical, hopeless and feel alone. it flipflops multiple times a day. my newest thing has been an extremely irrational fear of death. i hate seeing actors die in movies - thinking about death at all. the very thought of not being alive scares me to the point that i lie in bed and beg not to think about it any longer. i finally talked to my father about this last year - he said he had no recollection of any of this and that he was sorry if he had ever done it. i was hoping that would help, but it didn\\'t. he went to desert storm a young man and from what i know, he suffered from something similar to ptsd himself for quite a while. my mother on the otherhand - and my grandmother on her side, tell me to just get over it. it all happened in the past and that there\\'s nothing i can do so i shouldn\\'t bother worrying about it. i feel like i\\'ve slowly been traversing down a dark lonely path by myself for a long while - my doctor referred me to a psychiatrist once (big mistake) and all he did was try to force medication down my throat that made it even worse. i\\'d like to reiterate that i\\'m not suicidal but right now i\\'m failing miserably and i need to get a job, but i can\\'t find the strength any more to keep pretending that i am okay. so i ask this subreddit - what are the steps to take? i\\'m currently unemployed with no health insurance. i know that if i get another job i will just end up leaving it. i\\'ve \"enjoyed\" (didn\\'t dislike, double negatives, oof) the jobs i\\'ve had, and some were pretty good - the people like me, so i had no reason to leave. what is the equivalent of my best friend has also gone so far as to label me as a psychopath..:/ i\\'m just looking for similar experiences and mostly understanding of the situation (maybe guidance in what kind of people to aim to talk to, or maybe steps to take to help in diagnosis, etc) - so far in my life every single person has pretty much told me to harden the \"f\" up and get the heck over it. thanks.. **edit 2** i forgot to say that my father is a completely different person now - but when i see him so much as take a sip of alcohol it all floods back to me and i get extremely terrified and nervous.',\n",
              "       \"hating myself. wanting to hide under a rock. but when i push through those tough points and going through my day and doing what i need to do feels better. still don't feel great but i'm surviving.\",\n",
              "       \"i'd slowly let myself slip away, and reviled in filth for the better part of the year. i don't know why i chose to do this now, but i've never felt better.\",\n",
              "       'sorry in advance for the wall of text. but i really need some opinions and to type this all out. before i had the mental health nurse, i only went to a doc in the box at my uni for my adderall xr. for awhile, i chased the euphoria as what it was supposed to do (instead of letting tolerance settle), so i kept getting stepped up from adderall ir 10mg all the way to adderall xr 30mg + 10 mg adderall ir. after awhile, i started having frustration issues, depressed mood swings (both i never had before adderall), and lack of focus so i went to the mental health nurse for cbt and to change to either just 2 lower dose ir or another less strong stimulant. here\\'s where it went a little off. she said that my mood swings and compulsive thinking about how focused/aware i was was a sign of ocd and bipolar (everyone around me will tell you i\\'m nowhere near that) and, added to my depression, were creating my adhd symptoms. in addition, she said that the adderall would cause my adhd symptoms to get worse over time and cause damage to my neurons, so i should stop immediately. to treat my \"ocd\", \"bipolar,\" and depression, she gave me lithium carbonate, a mood stabilizer. i voiced my concerns, but then she said my hesitation to start it was another example of ocd. fine, i\\'ll try it. a week later, and i couldn\\'t even show excitement or be interested in talking to my friends, let alone match my work output that i did on adderall. she told me that it was due to being off adderall, but i\\'ve voluntarily been off adderall for a month before and i\\'ve never felt like this. i voiced my concerns and she just condescendingly asked me if i just wanted to see someone else to get stimulants that were going to damage me (i never even asked for the stimulants. i was just worried about the side effects). could i be wrong? another side note is that i went so i could also do cbt, but so far for a month and a half, we\\'ve only discussed medications for the 45 minute sessions despite my constant requests for cbt. and because i forgot to read a chapter of a book before coming in for one appointment, she has said \"i\\'m not organized enough to start\" ever since. that\\'s why i\\'m there! am i losing my mind or is she not making sense? edit: i am in the us and am currently looking at other options for psychiatrists and therapists. i thought a psych nurse would really help by doing both, but it turns out she\\'s trying to be more of a psychiatrist rather than a psych nurse. trying to argue my case with my head being so foggy has been really hard and seeing your comments helps so much.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display first 5 elements of labels"
      ],
      "metadata": {
        "id": "B2fyj1jr9peD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets.values[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBGBSS-3joxd",
        "outputId": "bdcec89b-5b83-4934-f327-f3e36f9ddce1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 4, 1, 3, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the data stored in Pandas Data Frame into a data stored in TensorFlow Data Set"
      ],
      "metadata": {
        "id": "HRY2wFRr0Ul-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train X & y\n",
        "train_post_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(train_features.values, tf.string)\n",
        ") \n",
        "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(train_targets.values, tf.int64),\n",
        "\n",
        ") \n",
        "# test X & y\n",
        "test_post_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(test_features.values, tf.string)\n",
        ") \n",
        "test_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(test_targets.values, tf.int64),\n",
        "\n",
        ") "
      ],
      "metadata": {
        "id": "OLdWWA0zjtxr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the dictionary size and the sequence length"
      ],
      "metadata": {
        "id": "M4O3Nug90eId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 500  # Only consider the top 20K words\n",
        "max_len = 250  # Maximum review (text) size in words\n",
        "embedding_dim = 16"
      ],
      "metadata": {
        "id": "Oj1oytRpjygc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess text"
      ],
      "metadata": {
        "id": "BUqmpSHrj10S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that will preprocess the text. It will perform the following:\n",
        "\n",
        "* Convert all characters to lowercase\n",
        "\n",
        "* Remove special symbols, extra spaces, html tags, digits, and punctuations\n",
        "\n",
        "* Remove stop words\n",
        "\n",
        "* Replace the special Turkish letters with the corresponding English letters."
      ],
      "metadata": {
        "id": "XZhI4hBfj4rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "#@tf.keras.utils.register_keras_serializable()\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
        "    no_uppercased = tf.strings.lower(input_string, encoding='utf-8')\n",
        "    no_stars = tf.strings.regex_replace(no_uppercased, \"\\*\", \" \")\n",
        "    no_repeats = tf.strings.regex_replace(no_stars, \"devamını oku\", \"\")    \n",
        "    no_html = tf.strings.regex_replace(no_repeats, \"<br />\", \"\")\n",
        "    no_digits = tf.strings.regex_replace(no_html, \"\\w*\\d\\w*\",\"\")\n",
        "    no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
        "    #remove stop words\n",
        "    no_stop_words = ' '+no_punctuations+ ' '\n",
        "    for each in stop_words:\n",
        "      no_stop_words = tf.strings.regex_replace(no_stop_words, ' '+each[0]+' ' , r\" \")\n",
        "    no_extra_space = tf.strings.regex_replace(no_stop_words, \" +\",\" \")\n",
        "\n",
        "    return no_extra_space"
      ],
      "metadata": {
        "id": "641nvi8Sj9U6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify if the `custom_standardization` function is working "
      ],
      "metadata": {
        "id": "ApGSjXQC1mIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_string = \"Bu Issız Öğlenleyin de;  şunu ***1 Pijamalı Hasta***, ve  Ancak İşte Yağız Şoföre Çabucak Güvendi...Devamını oku\"\n",
        "print(\"input:  \", input_string)\n",
        "output_string= custom_standardization(input_string)\n",
        "print(\"output: \", output_string.numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8YTBG7VkG15",
        "outputId": "974a85b2-e748-4b29-e239-0517d4b06c68"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:   Bu Issız Öğlenleyin de;  şunu ***1 Pijamalı Hasta***, ve  Ancak İşte Yağız Şoföre Çabucak Güvendi...Devamını oku\n",
            "output:   bu issız öğlenleyin de şunu pijamalı hasta ve ancak i̇şte yağız şoföre çabucak güvendi \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize/Vectorize words\n",
        "Tokenize words using the Keras `textVectorization()` function"
      ],
      "metadata": {
        "id": "V_i4S_xm11Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_len\n",
        ")"
      ],
      "metadata": {
        "id": "qvRdZTp0kpQg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenize/Adapt text vectorization layer with the training data set\n",
        "* Create a vocabulary of words"
      ],
      "metadata": {
        "id": "V1F-d0XF2Gns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(train_features)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
      ],
      "metadata": {
        "id": "jveq7lBMkrSl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display sample tokenization/vectorization"
      ],
      "metadata": {
        "id": "RXjJ4a4d2kG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocab has the \", len(vocab),\" entries\")\n",
        "print(\"vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \", vocab[word])\n",
        "\n",
        "for X in train_features[:1]:\n",
        "  print(\" Given raw data: \" )\n",
        "  print(X)\n",
        "  tokenized = vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers: \" )\n",
        "  print (tokenized)\n",
        "  print(\" Text after Tokenized and Transformed: \")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ vocab[each]\n",
        "  print(transformed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0fWnrWMkuOh",
        "outputId": "085245db-1e86-48eb-ebd1-510fa2a622e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab has the  500  entries\n",
            "vocab has the following first 10 entries\n",
            "0  represents the word:  \n",
            "1  represents the word:  [UNK]\n",
            "2  represents the word:  to\n",
            "3  represents the word:  and\n",
            "4  represents the word:  the\n",
            "5  represents the word:  my\n",
            "6  represents the word:  of\n",
            "7  represents the word:  it\n",
            "8  represents the word:  that\n",
            "9  represents the word:  in\n",
            " Given raw data: \n",
            "i was wondering where i could find an hourly dataset of a climate station of a specific county. the ideal dataset will be **hourly**, and will have information like wind speed, wind direction, atmospheric pressure, humidity, temperature, cloud type, and rainfall. i know that there are climate stations that collect this data but i can only find monthly or daily datasets.\n",
            " Tokenized and Transformed to a vector of integers: \n",
            "tf.Tensor(\n",
            "[[ 16   1 118  96 170  54   1   1   6   1   1   6   1   1   4   1   1  68\n",
            "   23   1   3  68  17   1  22   1   1   1   1   1   1   1   1   1   1   3\n",
            "    1  43   8  60  34   1   1   8   1  14 453  15  26  92 170   1  25   1\n",
            "    1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]], shape=(1, 250), dtype=int64)\n",
            " Text after Tokenized and Transformed: \n",
            " was [UNK] where could find an [UNK] [UNK] of [UNK] [UNK] of [UNK] [UNK] the [UNK] [UNK] will be [UNK] and will have [UNK] like [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] know that there are [UNK] [UNK] that [UNK] this data but can only find [UNK] or [UNK] [UNK]                                                                                                                                                                                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJhD4gPplQKB",
        "outputId": "667721ae-5cda-4436-c5a9-13abb8b74ed5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'to', 'and', 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Keras Text Vectorization to the training and test data sets"
      ],
      "metadata": {
        "id": "n_LKatol2uLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function `convert_text_input()` that apply text vectorization/tokenization to all posts"
      ],
      "metadata": {
        "id": "gtsG10LS-MXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_text_input(sample):\n",
        "    text = sample\n",
        "    text = tf.expand_dims(text, -1)  \n",
        "    return tf.squeeze(vectorize_layer(text))"
      ],
      "metadata": {
        "id": "6SSbwMEwluJG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tensor flow `map()` function was used to apply the `convert_text_input()` function on every `posts` of the training data set"
      ],
      "metadata": {
        "id": "_LDPlLnk24T0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode/tokenize raw text posts `train_post_ds_raw` and `test_post_ds`"
      ],
      "metadata": {
        "id": "cYmgIoFA7Y_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train X\n",
        "train_post_ds = train_post_ds_raw.map(convert_text_input, \n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "# Test X\n",
        "test_post_ds = test_post_ds_raw.map(convert_text_input, \n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "RRaxvomVlx3j"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check three tokenized/vectorized sample posts"
      ],
      "metadata": {
        "id": "BF2HRnwp5x-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for each in train_post_ds.take(3):\n",
        "  print(each)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EenWwxIBlzk7",
        "outputId": "9ba5b5df-3bd9-4cb7-fc80-87897d232add"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 16   1 118  96 170  54   1   1   6   1   1   6   1   1   4   1   1  68\n",
            "  23   1   3  68  17   1  22   1   1   1   1   1   1   1   1   1   1   3\n",
            "   1  43   8  60  34   1   1   8   1  14 453  15  26  92 170   1  25   1\n",
            "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(250,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[ 20  45 271   1  36 224   4 218   8   5 320 101   1  14  16   9   1   6\n",
            "  16  92   1  90 283 322   8   1   1   9   4   1 444   6  91   4   1  12\n",
            "  39  75 260   6 183   1   5 474   3   1 109 327   1  40 101   1 357   1\n",
            " 386 260   6   1 151   7  16   2  98  10 401  36 224  39  16 140 151   8\n",
            " 110   1 322   5   1 414   3   1 128   6   1  75   5   1  66  37   1  35\n",
            "   1  39  48 126  66  37   1  14  16   1  41   6 127   1   3   1   1  10\n",
            "   7 334   1  11  10   2  23   1  18   1   1   1   3   4  22  28  54   1\n",
            " 128   6   4  42  48  66   1   5 310   1   1   3 194  10   8  48  96   1\n",
            "   5 448  35  20   1  48  16   1   1   1 438  10   8  48   1  45  89  46\n",
            "   1   8  16   1   1   3   8  48 198   2   1  10   7 298  20 325   8  60\n",
            "   1  87   1   1   6   7   1 391   2  71   1 386 389  14  66 412   9   4\n",
            "   1   4   1  25   9 142   1   7  89 298  19   4 124  25 490   1   6   4\n",
            "   1   1  55  42 118  48  16   1   1  27   4   1   1   1  48  66   1  30\n",
            "   6 177 199   1 364  27   4   1  27   4  42 368   4   1  40   1], shape=(250,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  1  64   1   2   1 477   1  15  39   1 116 193   1   1   3  77 116   5\n",
            "  78   3 134  38 115   2  31 275 129  99  51  33 264  15   1   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(250,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalize the training data set \n"
      ],
      "metadata": {
        "id": "PaIKolakl7kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the preprocessed and encoded posts `train_post_ds` with the encoded categories `train_cat_ds_raw`"
      ],
      "metadata": {
        "id": "qs_OYpqv-qSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.zip(\n",
        "    (\n",
        "            train_post_ds,\n",
        "            train_cat_ds_raw\n",
        "     )\n",
        ") "
      ],
      "metadata": {
        "id": "RLtfabYYl-dA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge the preprocessed and encoded posts `test_post_ds` with the encoded categories `test_cat_ds_raw`"
      ],
      "metadata": {
        "id": "LXYUp2lPmC8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = tf.data.Dataset.zip(\n",
        "    (\n",
        "            test_post_ds,\n",
        "            test_cat_ds_raw\n",
        "     )\n",
        ") "
      ],
      "metadata": {
        "id": "MbUvnBCEl_-W"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display one post from `train_ds`"
      ],
      "metadata": {
        "id": "ff7BsorrmbBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for X,y in train_ds.take(1):\n",
        "  print(\"input (review) X.shape: \", X.shape)\n",
        "  print(\"output (category) y.shape: \", y.shape)\n",
        "  print(\"input (review) X: \", X)\n",
        "  print(\"output (category) y: \",y)\n",
        "  input = \" \".join([vocab[_] for _ in np.squeeze(X)])\n",
        "  output = id_to_category[y.numpy()]\n",
        "  print(\"X: input (review) in text: \" , input)\n",
        "  print(\"y: output (category) in text: \" , output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auGFXUZymHQj",
        "outputId": "aef4c4ee-b956-4710-b1dd-b23da193e0e2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (review) X.shape:  (250,)\n",
            "output (category) y.shape:  ()\n",
            "input (review) X:  tf.Tensor(\n",
            "[ 16   1 118  96 170  54   1   1   6   1   1   6   1   1   4   1   1  68\n",
            "  23   1   3  68  17   1  22   1   1   1   1   1   1   1   1   1   1   3\n",
            "   1  43   8  60  34   1   1   8   1  14 453  15  26  92 170   1  25   1\n",
            "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(250,), dtype=int64)\n",
            "output (category) y:  tf.Tensor(5, shape=(), dtype=int64)\n",
            "X: input (review) in text:  was [UNK] where could find an [UNK] [UNK] of [UNK] [UNK] of [UNK] [UNK] the [UNK] [UNK] will be [UNK] and will have [UNK] like [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] know that there are [UNK] [UNK] that [UNK] this data but can only find [UNK] or [UNK] [UNK]                                                                                                                                                                                                   \n",
            "y: output (category) in text:  none\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalize tensorflow data pipeline"
      ],
      "metadata": {
        "id": "0xb4Q-oE7oFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Set batch size\n",
        "* Shuffle data set\n",
        "* Optimize"
      ],
      "metadata": {
        "id": "01Q6FsrE7upD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "buffer_size= train_ds.cardinality().numpy()\n",
        "\n",
        "train_ds = train_ds.shuffle(buffer_size=buffer_size)\\\n",
        "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
        "                   .cache()\\\n",
        "                   .prefetch(AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.shuffle(buffer_size=buffer_size)\\\n",
        "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
        "                   .cache()\\\n",
        "                   .prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "subV5tVMmcyw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di-FU2OAmfzB",
        "outputId": "e57e3b8e-1378-4755-a479-288e3ffd5bb4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(64,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create classification model"
      ],
      "metadata": {
        "id": "H8qZxeHz8A5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding only"
      ],
      "metadata": {
        "id": "Gni8jDoDxC54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(len(vocab), 64, input_length=max_len),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(6)\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "\n",
        "history = model.fit(train_ds, epochs=30, validation_data=test_ds)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Train accuracy1: \", accuracy)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJl6fhe-Uyf_",
        "outputId": "33354a80-1ec9-4334-b85e-b57aa064ce02"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "190/190 [==============================] - 3s 10ms/step - loss: 1.7322 - sparse_categorical_accuracy: 0.2554 - val_loss: 1.5996 - val_sparse_categorical_accuracy: 0.3384\n",
            "Epoch 2/30\n",
            "190/190 [==============================] - 2s 11ms/step - loss: 1.5365 - sparse_categorical_accuracy: 0.3580 - val_loss: 1.4727 - val_sparse_categorical_accuracy: 0.3933\n",
            "Epoch 3/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 1.4460 - sparse_categorical_accuracy: 0.4149 - val_loss: 1.3854 - val_sparse_categorical_accuracy: 0.4528\n",
            "Epoch 4/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 1.3431 - sparse_categorical_accuracy: 0.4909 - val_loss: 1.2698 - val_sparse_categorical_accuracy: 0.5329\n",
            "Epoch 5/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 1.2146 - sparse_categorical_accuracy: 0.5604 - val_loss: 1.1473 - val_sparse_categorical_accuracy: 0.6011\n",
            "Epoch 6/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 1.0928 - sparse_categorical_accuracy: 0.6229 - val_loss: 1.0455 - val_sparse_categorical_accuracy: 0.6326\n",
            "Epoch 7/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.9970 - sparse_categorical_accuracy: 0.6599 - val_loss: 0.9739 - val_sparse_categorical_accuracy: 0.6626\n",
            "Epoch 8/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.9254 - sparse_categorical_accuracy: 0.6834 - val_loss: 0.9254 - val_sparse_categorical_accuracy: 0.6759\n",
            "Epoch 9/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.8767 - sparse_categorical_accuracy: 0.6966 - val_loss: 0.8953 - val_sparse_categorical_accuracy: 0.6825\n",
            "Epoch 10/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.8480 - sparse_categorical_accuracy: 0.7077 - val_loss: 0.8735 - val_sparse_categorical_accuracy: 0.6971\n",
            "Epoch 11/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.8262 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.8638 - val_sparse_categorical_accuracy: 0.7025\n",
            "Epoch 12/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.8077 - sparse_categorical_accuracy: 0.7205 - val_loss: 0.8512 - val_sparse_categorical_accuracy: 0.7051\n",
            "Epoch 13/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7874 - sparse_categorical_accuracy: 0.7262 - val_loss: 0.8422 - val_sparse_categorical_accuracy: 0.7101\n",
            "Epoch 14/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7736 - sparse_categorical_accuracy: 0.7328 - val_loss: 0.8390 - val_sparse_categorical_accuracy: 0.7088\n",
            "Epoch 15/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7643 - sparse_categorical_accuracy: 0.7328 - val_loss: 0.8388 - val_sparse_categorical_accuracy: 0.7064\n",
            "Epoch 16/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7594 - sparse_categorical_accuracy: 0.7349 - val_loss: 0.8336 - val_sparse_categorical_accuracy: 0.7104\n",
            "Epoch 17/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7477 - sparse_categorical_accuracy: 0.7398 - val_loss: 0.8322 - val_sparse_categorical_accuracy: 0.7104\n",
            "Epoch 18/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7476 - sparse_categorical_accuracy: 0.7388 - val_loss: 0.8348 - val_sparse_categorical_accuracy: 0.7061\n",
            "Epoch 19/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7382 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.8312 - val_sparse_categorical_accuracy: 0.7128\n",
            "Epoch 20/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7322 - sparse_categorical_accuracy: 0.7433 - val_loss: 0.8322 - val_sparse_categorical_accuracy: 0.7101\n",
            "Epoch 21/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7257 - sparse_categorical_accuracy: 0.7470 - val_loss: 0.8335 - val_sparse_categorical_accuracy: 0.7121\n",
            "Epoch 22/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7297 - sparse_categorical_accuracy: 0.7466 - val_loss: 0.8322 - val_sparse_categorical_accuracy: 0.7121\n",
            "Epoch 23/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7183 - sparse_categorical_accuracy: 0.7491 - val_loss: 0.8332 - val_sparse_categorical_accuracy: 0.7121\n",
            "Epoch 24/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7181 - sparse_categorical_accuracy: 0.7486 - val_loss: 0.8342 - val_sparse_categorical_accuracy: 0.7151\n",
            "Epoch 25/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7136 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.8366 - val_sparse_categorical_accuracy: 0.7134\n",
            "Epoch 26/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7138 - sparse_categorical_accuracy: 0.7497 - val_loss: 0.8376 - val_sparse_categorical_accuracy: 0.7134\n",
            "Epoch 27/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7071 - sparse_categorical_accuracy: 0.7521 - val_loss: 0.8394 - val_sparse_categorical_accuracy: 0.7114\n",
            "Epoch 28/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7100 - sparse_categorical_accuracy: 0.7525 - val_loss: 0.8363 - val_sparse_categorical_accuracy: 0.7131\n",
            "Epoch 29/30\n",
            "190/190 [==============================] - 1s 6ms/step - loss: 0.7081 - sparse_categorical_accuracy: 0.7544 - val_loss: 0.8393 - val_sparse_categorical_accuracy: 0.7081\n",
            "Epoch 30/30\n",
            "190/190 [==============================] - 1s 7ms/step - loss: 0.7043 - sparse_categorical_accuracy: 0.7509 - val_loss: 0.8422 - val_sparse_categorical_accuracy: 0.7114\n",
            "47/47 [==============================] - 0s 3ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.7114\n",
            "Train accuracy1:  0.7114361524581909\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 250, 64)           32000     \n",
            "                                                                 \n",
            " global_average_pooling1d_4   (None, 64)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,550\n",
            "Trainable params: 36,550\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"sparse_categorical_accuracy\")\n",
        "plot_graphs(history, \"loss\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_JJhoiRiXFqV",
        "outputId": "d14553cf-faf8-4ad3-e106-d75f3c6da54d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def plot_graphs(history, string):\\n  plt.plot(history.history[string])\\n  plt.plot(history.history[\\'val_\\'+string])\\n  plt.xlabel(\"Epochs\")\\n  plt.ylabel(string)\\n  plt.legend([string, \\'val_\\'+string])\\n  plt.show()\\n  \\nplot_graphs(history, \"sparse_categorical_accuracy\")\\nplot_graphs(history, \"loss\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data=['i hate myself',\n",
        "          'i do not like myself']\n",
        "predictions=end_to_end_model.predict(raw_data)\n",
        "print(id_to_category[np.argmax(predictions[0])])\n",
        "print(id_to_category[np.argmax(predictions[1])])"
      ],
      "metadata": {
        "id": "-jDnC-CaqLPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2983e02c-37b1-49db-854f-80bd32803362"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-11525251ce86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m raw_data=['i hate myself',\n\u001b[1;32m      2\u001b[0m           'i do not like myself']\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_to_end_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_category\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_to_category\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'end_to_end_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bidirectional LSTMs"
      ],
      "metadata": {
        "id": "5vVLzLnOy3J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "embedding_dim = 64\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(vocab), embedding_dim, input_length=max_len),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(6)\n",
        "])\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "\n",
        "history = model.fit(train_ds, epochs=epochs, validation_data=test_ds)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Train accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzMfLrdKxa0S",
        "outputId": "dfe28c2d-70ac-43fd-d6bc-f1d3e64dbcca"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 30s 136ms/step - loss: 1.7870 - sparse_categorical_accuracy: 0.1994 - val_loss: 1.7703 - val_sparse_categorical_accuracy: 0.2284\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.6950 - sparse_categorical_accuracy: 0.2680 - val_loss: 1.6502 - val_sparse_categorical_accuracy: 0.2955\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.6372 - sparse_categorical_accuracy: 0.2934 - val_loss: 1.5935 - val_sparse_categorical_accuracy: 0.3062\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.5880 - sparse_categorical_accuracy: 0.3005 - val_loss: 1.5487 - val_sparse_categorical_accuracy: 0.3122\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 25s 133ms/step - loss: 1.5588 - sparse_categorical_accuracy: 0.3154 - val_loss: 1.5214 - val_sparse_categorical_accuracy: 0.3331\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.5348 - sparse_categorical_accuracy: 0.3251 - val_loss: 1.4989 - val_sparse_categorical_accuracy: 0.3501\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.5086 - sparse_categorical_accuracy: 0.3382 - val_loss: 1.4811 - val_sparse_categorical_accuracy: 0.3577\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.4949 - sparse_categorical_accuracy: 0.3464 - val_loss: 1.4707 - val_sparse_categorical_accuracy: 0.3610\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 25s 132ms/step - loss: 1.4783 - sparse_categorical_accuracy: 0.3605 - val_loss: 1.4546 - val_sparse_categorical_accuracy: 0.3747\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 25s 133ms/step - loss: 1.4580 - sparse_categorical_accuracy: 0.3663 - val_loss: 1.4256 - val_sparse_categorical_accuracy: 0.3999\n",
            "47/47 [==============================] - 2s 36ms/step - loss: 1.4256 - sparse_categorical_accuracy: 0.3999\n",
            "Train accuracy:  0.39993351697921753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(vocab), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(6)\n",
        "])\n",
        "\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "\n",
        "history = model.fit(train_ds, epochs=epochs, validation_data=test_ds)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Train accuracy: \", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA3cpMmuJLGg",
        "outputId": "c45bd2e2-1d04-44b1-e746-892eb68acc5b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "190/190 [==============================] - 68s 270ms/step - loss: 1.7634 - sparse_categorical_accuracy: 0.2318 - val_loss: 1.6016 - val_sparse_categorical_accuracy: 0.2889\n",
            "Epoch 2/10\n",
            "190/190 [==============================] - 46s 240ms/step - loss: 1.5863 - sparse_categorical_accuracy: 0.3086 - val_loss: 1.5263 - val_sparse_categorical_accuracy: 0.3251\n",
            "Epoch 3/10\n",
            "190/190 [==============================] - 46s 240ms/step - loss: 1.5271 - sparse_categorical_accuracy: 0.3371 - val_loss: 1.4835 - val_sparse_categorical_accuracy: 0.3514\n",
            "Epoch 4/10\n",
            "190/190 [==============================] - 45s 239ms/step - loss: 1.4720 - sparse_categorical_accuracy: 0.3639 - val_loss: 1.4258 - val_sparse_categorical_accuracy: 0.3876\n",
            "Epoch 5/10\n",
            "190/190 [==============================] - 46s 240ms/step - loss: 1.4126 - sparse_categorical_accuracy: 0.3906 - val_loss: 1.3790 - val_sparse_categorical_accuracy: 0.4156\n",
            "Epoch 6/10\n",
            "190/190 [==============================] - 46s 240ms/step - loss: 1.3640 - sparse_categorical_accuracy: 0.4076 - val_loss: 1.3408 - val_sparse_categorical_accuracy: 0.4372\n",
            "Epoch 7/10\n",
            "190/190 [==============================] - 45s 239ms/step - loss: 1.3286 - sparse_categorical_accuracy: 0.4204 - val_loss: 1.3158 - val_sparse_categorical_accuracy: 0.4435\n",
            "Epoch 8/10\n",
            "190/190 [==============================] - 45s 239ms/step - loss: 1.2982 - sparse_categorical_accuracy: 0.4401 - val_loss: 1.2966 - val_sparse_categorical_accuracy: 0.4614\n",
            "Epoch 9/10\n",
            "190/190 [==============================] - 45s 239ms/step - loss: 1.2668 - sparse_categorical_accuracy: 0.4601 - val_loss: 1.2663 - val_sparse_categorical_accuracy: 0.4804\n",
            "Epoch 10/10\n",
            "190/190 [==============================] - 45s 239ms/step - loss: 1.2399 - sparse_categorical_accuracy: 0.4817 - val_loss: 1.2421 - val_sparse_categorical_accuracy: 0.4937\n",
            "47/47 [==============================] - 4s 82ms/step - loss: 1.2421 - sparse_categorical_accuracy: 0.4937\n",
            "Train accuracy:  0.49368351697921753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "QQnJ2D2D9KHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "embedding_dim = 16\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    tf.keras.layers.Conv1D(16, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.0001\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "\n",
        "history = model.fit(train_ds, verbose=1, epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Train accuracy: \", accuracy)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  keras.Input(shape=(1,), dtype=\"string\"),\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(test_features, test_targets)\n",
        "print(\"Train accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "bdckrVes0H6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "jGdko_L_9M9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "    tf.keras.layers.Dense(6, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.00003\n",
        "\n",
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "              metrics=['SparseCategoricalAccuracy'])\n",
        "\n",
        "history = model.fit(train_ds, verbose=1, epochs=epochs)\n",
        "\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Train accuracy: \", accuracy)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  keras.Input(shape=(1,), dtype=\"string\"),\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(test_features, test_targets)\n",
        "print(\"Train accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "jOambrDC03mP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}